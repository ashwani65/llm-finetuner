# MLflow Configuration
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_EXPERIMENT_NAME=llm-finetuning

# Weights & Biases
WANDB_API_KEY=your_wandb_api_key_here
WANDB_PROJECT=llm-finetuner
WANDB_ENTITY=your_username_or_team

# HuggingFace
HF_TOKEN=your_huggingface_token_here
HF_HOME=./cache/huggingface

# Model Configuration
BASE_MODEL_NAME=meta-llama/Llama-2-7b-hf
# Alternative: mistralai/Mistral-7B-v0.1

# Training Configuration
MAX_SEQ_LENGTH=512
BATCH_SIZE=4
GRADIENT_ACCUMULATION_STEPS=4
LEARNING_RATE=2e-4
NUM_EPOCHS=3

# LoRA Configuration
LORA_R=16
LORA_ALPHA=32
LORA_DROPOUT=0.05

# Quantization
LOAD_IN_4BIT=true
QUANTIZATION_TYPE=nf4

# Paths
DATA_DIR=./data
MODEL_OUTPUT_DIR=./models
LOGS_DIR=./logs

# GPU Configuration
CUDA_VISIBLE_DEVICES=0
TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0

# Deployment
VLLM_HOST=0.0.0.0
VLLM_PORT=8000
TENSOR_PARALLEL_SIZE=1

# AWS (Optional)
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key
# AWS_REGION=us-east-1
# S3_BUCKET=your_bucket_name

# GCP (Optional)
# GOOGLE_APPLICATION_CREDENTIALS=path/to/credentials.json
# GCP_PROJECT_ID=your_project_id
# GCS_BUCKET=your_bucket_name
