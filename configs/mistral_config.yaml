# Mistral 7B Configuration

model:
  name: "mistralai/Mistral-7B-v0.1"
  max_seq_length: 512
  cache_dir: "./cache/huggingface"

lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

training:
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  warmup_steps: 100
  weight_decay: 0.01
  max_grad_norm: 1.0
  optim: "paged_adamw_32bit"
  lr_scheduler_type: "cosine"
  save_steps: 100
  save_total_limit: 3
  eval_steps: 100
  logging_steps: 10
  fp16: true
  gradient_checkpointing: true
  output_dir: "./models/mistral-v1"

data:
  train_file: "./data/train.json"
  validation_file: "./data/validation.json"
  test_file: "./data/test.json"
  max_length: 512

tracking:
  experiment_name: "mistral-finetuning"
  mlflow_uri: "http://localhost:5000"
  use_wandb: false

inference:
  max_new_tokens: 128
  temperature: 0.1
  top_p: 0.95
  do_sample: false
