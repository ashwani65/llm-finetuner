# Base Configuration for LLM Fine-tuning

model:
  name: "meta-llama/Llama-2-7b-hf"
  max_seq_length: 512
  cache_dir: "./cache/huggingface"

# LoRA Configuration
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

# Quantization Configuration
quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# Training Configuration
training:
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  warmup_steps: 100
  weight_decay: 0.01
  max_grad_norm: 1.0

  # Optimization
  optim: "paged_adamw_32bit"
  lr_scheduler_type: "cosine"

  # Checkpointing
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 3

  # Evaluation
  evaluation_strategy: "steps"
  eval_steps: 100
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"

  # Logging
  logging_steps: 10
  logging_strategy: "steps"

  # Memory Optimization
  fp16: true
  gradient_checkpointing: true

  # Output
  output_dir: "./output"
  overwrite_output_dir: true

# Data Configuration
data:
  train_file: "./data/train.json"
  validation_file: "./data/validation.json"
  test_file: "./data/test.json"

  # Split ratios (if generating from single file)
  train_ratio: 0.7
  validation_ratio: 0.15
  test_ratio: 0.15

  # Preprocessing
  max_length: 512
  padding: "max_length"
  truncation: true

# Experiment Tracking
tracking:
  experiment_name: "llm-finetuning"
  run_name: null  # Auto-generated if null
  mlflow_uri: "http://localhost:5000"
  use_wandb: false
  wandb_project: "llm-finetuner"

# Inference Configuration
inference:
  max_new_tokens: 128
  temperature: 0.1
  top_p: 0.95
  do_sample: false
  num_beams: 1
